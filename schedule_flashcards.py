print("Importing pandas and matplotlib")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print("Importing sklearn and seaborn")
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
import seaborn as sns
print('Importing keras utilities')
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.metrics import Recall, Precision, AUC, Metric
from keras import backend as K
from keras.callbacks import EarlyStopping
print("Imported.")


# use this to predict fall off in review success by day
#   if it's mainly relying on card or deck success rate then it will not do so well

# TODO: publish add on updating quantified self to export
#   data from anki that includes deck id

# scale to [-1, 1] which seems to have better results
# https://stats.stackexchange.com/questions/364735/why-does-0-1-scaling
#   -dramatically-increase-training-time-for-feed-forward-an/364776#364776

# explore batch normalization
#   https://arxiv.org/abs/1806.02375v1

# study this
#   https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my
#   -neural-network-doesnt-learn

# try a getting started problem on kaggle
#    https://elitedatascience.com/beginner-kaggle

# target chance of performance across random card in deck
# select the review that will most increase that performance tomorrow
# but that's going to always be an unreviewed card?

# study hyperparameter tuning:
# https://www.analyticsvidhya.com/blog/2021/05/
#   tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/
# https://towardsdatascience.com/supervised-machine-learning-feature
#   -engineering-and-hyper-parameter-tuning-a3da583dd7b9
# https://arxiv.org/pdf/1206.5533v2.pdf

# research "half-life regression" and Mozer/Duolingo/ebisu:
# https://fasiha.github.io/ebisu/

def process_review_history(filename):
    '''Process the .csv export generated by Quantified Self add on into a
    pandas dataframe. The rows in this csv contain a span between reviews,
    so it lists review1 and review2, then the next row might be review2 and
    review3. Some processing is needed to ensure we capture each review,
    since most of them show up in the data twice, EXCEPT the first and last
    reviews only show up once, and in separate columns.
    '''
    df = pd.read_csv(filename)

    # create two dataframes from Date1, Answer1 and Date2, Answer2
    df_1 = df[['Date1', 'Answer1', 'Card ID']].copy()
    df_2 = df[['Date2', 'Answer2', 'Card ID']].copy()

    # rename columns to have same column names
    df_1.columns = ['Date', 'Answer', 'Card ID']
    df_2.columns = ['Date', 'Answer', 'Card ID']

    df_merged = pd.concat([df_1, df_2]).drop_duplicates()
    df_merged['Date'] = pd.to_datetime(df_merged['Date'])
    df_merged = df_merged.sort_values(['Card ID', 'Date'])
    df_merged = df_merged.reset_index(drop=True)

    return df_merged


def add_features(df):
    """Add additional features derived from collected data.
    """
    # Add Last date and interval
    df['Last Date'] = df.groupby('Card ID')['Date'].shift()
    df['Interval'] = df['Date'] - df['Last Date']

    # Add Last interval, Last result
    df['Last Interval'] = df.groupby('Card ID')['Interval'].shift()
    df['Last Answer'] = df.groupby('Card ID')['Answer'].shift()

    # Calculate the cumulative success rate.
    df['Successful'] = df['Answer'].apply(lambda x: 1 if x > 1 else 0)
    df['Cumulative Successes'] = df.groupby('Card ID')['Successful'].cumsum()
    df['Cumulative Reviews'] = df.groupby('Card ID').cumcount()

    # Shift the cumulative columns by 1 step for each 'Card ID', add a 0
    df['Cumulative Successes'] = (
        df.groupby('Card ID')['Cumulative Successes'].shift(fill_value=0))
    df['Cumulative Reviews'] = (
        df.groupby('Card ID')['Cumulative Reviews'].shift(fill_value=0))

    df['Cumulative Success Rate'] = (
        df['Cumulative Successes'] / df['Cumulative Reviews'])

    # Add smoothed cumulative success rate
    # This might help minimize the issue of overconfidence at few reviews
    # TODO: tune additive smoothing values
    adsmooth_num = 0.85  # approx. collection overall success rate as prior
    adsmooth_den = 1
    df['Smoothed Success Rate'] = (
        (df['Cumulative Successes'] + adsmooth_num)
        / (df['Cumulative Reviews'] + adsmooth_den))

    # Add longest successful interval
    df['Longest Successful Interval'] = (
        df[df['Successful'] == True].groupby('Card ID')['Interval'].cummax()
    )

    df['Longest Successful Interval'] = (
        df.groupby('Card ID')['Longest Successful Interval']
        .fillna(method='ffill')
    )

    df['Longest Successful Interval'].fillna(pd.Timedelta(seconds=0),
                                             inplace=True)

    # Add shortest failed interval
    df['Shortest Failed Interval'] = (
        df[df['Successful'] == False].groupby('Card ID')['Interval'].cummin()
    )
    df['Shortest Failed Interval'] = (
        df.groupby('Card ID')['Shortest Failed Interval']
        .fillna(method='ffill')
    )

    return df


def normalize_features(df):
    """Converts pd dates to Unix epoch seconds and normalizes features"""

    # Convert pd NaTs to NaNs so log can handle them
    for col in ['Date',
                'Last Date',
                'Interval',
                'Last Interval',
                'Longest Successful Interval',
                'Shortest Failed Interval']:
        df[col] = df[col].replace({pd.NaT: np.nan})

    # Convert to Unix timestamps
    df['Date'] = df['Date'].apply(lambda x: x.timestamp())
    df['Last Date'] = (
        df['Last Date'].apply(lambda x: x.timestamp() if pd.notnull(x) else x))

    df['Interval'] = df['Interval'].dt.total_seconds()
    df['Interval'].replace(pd.NaT, np.nan, inplace=True)
    # log scale to compress long tail of values for very long reviews
    df['Interval'] = np.log(df['Interval'] + 0.000001)

    df['Last Interval'] = df['Last Interval'].dt.total_seconds()
    df['Last Interval'].replace(pd.NaT, np.nan, inplace=True)
    df['Last Interval'] = np.log(df['Last Interval'] + 0.000001)

    df['Longest Successful Interval'] = (
        df['Longest Successful Interval'].dt.total_seconds())
    df['Longest Successful Interval'].replace(pd.NaT, float(0), inplace=True)
    df['Longest Successful Interval'].fillna(float(0), inplace=True)
    df['Longest Successful Interval'] = np.log(
        df['Longest Successful Interval'] + 0.000001)
    print('Data Frame Types:')
    print(df.dtypes)
    print()

    df['Shortest Failed Interval'] = (
        df['Shortest Failed Interval'].dt.total_seconds())
    df['Shortest Failed Interval'].replace(pd.NaT, np.nan, inplace=True)
    df['Shortest Failed Interval'] = np.log(
        (df['Shortest Failed Interval'] + 0.000001))

    # Normalize to 0-1 range
    df['Date'] = (
        (df['Date'] - df['Date'].min())
        / (df['Date'].max() - df['Date'].min()))
    df['Last Date'] = (
        (df['Last Date'] - df['Last Date'].min())
        / (df['Last Date'].max() - df['Last Date'].min()))
    df['Interval'] = (
        (df['Interval'] - df['Interval'].min())
        / (df['Interval'].max() - df['Interval'].min()))
    df['Last Interval'] = (
        (df['Last Interval'] - df['Last Interval'].min())
        / (df['Last Interval'].max() - df['Last Interval'].min()))
    df['Longest Successful Interval'] = (
        (df['Longest Successful Interval']
            - df['Longest Successful Interval'].min())
        / (df['Longest Successful Interval'].max()
            - df['Longest Successful Interval'].min()))
    df['Shortest Failed Interval'] = (
        (df['Shortest Failed Interval']
            - df['Shortest Failed Interval'].min())
        / (df['Shortest Failed Interval'].max()
            - df['Shortest Failed Interval'].min()))

    answer_mapping = {1: 0, 2: 0.5, 3: 0.8, 4: 1}
    # df['Answer'] = df['Answer'].map(answer_mapping)
    df['Last Answer'] = df['Last Answer'].map(answer_mapping)

    return df


def plot_interval_success(df):
    # plotting relationship between interval and success
    # interval bins are poorly labeled but are 1-2 days, 2-4 days, 4-8, etc

    # Convert 'Interval' from Timedelta to seconds
    df['Interval_seconds'] = df['Interval'].dt.total_seconds()

    # Then, create bins for 'Interval_seconds'
    bin_size = 60*60*24  # bin size of 1 day
    # bins = np.arange(0, df['Interval_seconds'].max() + bin_size, bin_size)

    bins = [0, (60*60*24)]
    while bins[-1] < df['Interval_seconds'].max():
        # Double the last value in the list and append it
        bins.append(bins[-1]*2)
    bins = np.array(bins)

    df['Interval_binned'] = pd.cut(df['Interval_seconds'], bins)

    # Then, group by 'Interval_binned' and calculate success rate
    success_rate = df.groupby('Interval_binned')['Successful'].mean()

    # Plot the success rate for each bin
    plt.figure(figsize=(10, 6))
    success_rate.plot(kind='line')
    plt.title('Success Rate vs Interval')
    plt.xlabel('Interval (days)')
    plt.ylabel('Success Rate')

    plt.show()


df = process_review_history('RevisionHistory.csv')
print('CSV read.')
df = add_features(df)
print('Features added.')
df = normalize_features(df)
print('Features normalized.')

'''
NaT errors to resolve:
Last Date: NaT if 1st review of card
Interval: NaT if 1st rev of card
Last Answer: NaT if 1st rev of card
Last Interval: NaT if no prior Interval, 1st or 2nd rev of card
Shortest Failed Interval: NaT until first fail
'''
# resolve NaT issues by removing first review of each card,
# removing the shortest failure column
# and imputing the last interval as this interval for second reviews
# cast Last Date and Interval to floats after NaTs removed
df = df.dropna(subset=['Last Date'])
df['Last Interval'].fillna(df['Interval'], inplace=True)
df.drop('Shortest Failed Interval', axis=1, inplace=True)

df['Last Date'] = df['Last Date'].astype(float)
df['Interval'] = df['Interval'].astype(float)

print("NaT issues resolved.")

# Balance df
# Determine target ratio
successful_ratio = 0.49
unsuccessful_ratio = 1 - successful_ratio

# Count of 'unsuccessful' reviews
num_unsuccessful = df['Successful'].value_counts()[0]

# Calculate target number of 'successful' reviews based on the target ratio
target_successful = (
    int(num_unsuccessful * (successful_ratio / unsuccessful_ratio)))

# Get indices of 'successful' and 'unsuccessful' reviews
successful_indices = df[df['Successful'] == 1].index
unsuccessful_indices = df[df['Successful'] == 0].index

# Randomly select 'successful' reviews to reach target number
random_indices = (
    np.random.choice(successful_indices, target_successful, replace=False))

# Concatenate indices and create a balanced dataframe
under_sample_indices = np.concatenate([random_indices, unsuccessful_indices])
df_balanced = df.loc[under_sample_indices]
df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)

print(f"Dataframe balanced with {successful_ratio * 100}% successful reviews.")

df = df_balanced.copy()

columns_to_drop = [
    'Date',
    'Answer',
    'Card ID',
    'Last Date',
    'Cumulative Reviews',
    'Cumulative Success Rate']
    #  Don't Drop:
'''
'Cumulative Successes',
'Longest Successful Interval',
'Last Interval',
'Last Answer']'''

for c in columns_to_drop:
    df = df.drop(c, axis=1)

rows_to_show = 50
print(f'Top {rows_to_show} rows: \n{df.head(rows_to_show)}')

X = df.drop('Successful', axis=1)
y = df['Successful']

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42)

print("Training and testing sets split.")


# backup that might prevent neuron death
def relu_tanh(x):
    return tf.concat([tf.nn.leaky_relu(x), tf.nn.tanh(x)], axis=-1)


# Define a neural net model using keras
neural_model = Sequential()
neural_model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))
neural_model.add(Dense(24, activation='relu'))
neural_model.add(Dense(1, activation='sigmoid'))


# Define your custom Adam optimizer with a learning rate
custom_adam = Adam(learning_rate=0.00001)

# Define metrics
precision = Precision(name='precision')
recall = Recall(name='recall')


class F1Score(Metric):
    def __init__(self, name='f1_score', **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.precision = self.add_weight(name='tp', initializer='zeros')
        self.recall = self.add_weight(name='fp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        precision = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        recall = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        self.precision.assign(precision)
        self.recall.assign(recall)
        
    def result(self):
        p = self.precision
        r = self.recall
        return 2*((p*r)/(p+r+K.epsilon()))

    def reset_state(self):
        # The state of the metric will be reset at the start of each epoch.
        self.precision.assign(0.0)
        self.recall.assign(0.0)


class BalancedErrorRate(Metric):
    def __init__(self, name='balanced_error_rate', **kwargs):
        super(BalancedErrorRate, self).__init__(name=name, **kwargs)
        self.tp = self.add_weight(name='tp', initializer='zeros')
        self.tn = self.add_weight(name='tn', initializer='zeros')
        self.fp = self.add_weight(name='fp', initializer='zeros')
        self.fn = self.add_weight(name='fn', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
        fp = K.sum(K.round(K.clip((1-y_true) * y_pred, 0, 1)))
        fn = K.sum(K.round(K.clip(y_true * (1-y_pred), 0, 1)))
        self.tp.assign(tp)
        self.tn.assign(tn)
        self.fp.assign(fp)
        self.fn.assign(fn)
        
    def result(self):
        return 2 * K.maximum(self.fp, self.fn) / (self.tp + self.tn + self.fp + self.fn)

    def reset_state(self):
        # The state of the metric will be reset at the start of each epoch.
        self.tp.assign(0.0)
        self.tn.assign(0.0)
        self.fp.assign(0.0)
        self.fn.assign(0.0)

# Compile the model
neural_model.compile(loss='binary_crossentropy',
                     optimizer='adam',
                     metrics=['accuracy', F1Score()],
                     weighted_metrics=[BalancedErrorRate()])

# Weight
# If y_train is a pandas Series, convert it to a numpy array
if isinstance(y_train, pd.Series):
    y_train = y_train.values

class_weights = class_weight.compute_sample_weight('balanced', y_train)


# Fit the model
early_stop = EarlyStopping(monitor='val_loss', patience=10)

neural_model.fit(
    X_train,
    y_train,
    epochs=500,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stop],
    sample_weight=class_weights)

# Evaluate the model
nn_loss, nn_accuracy, nn_f1, nn_balanced_error_rate = (
    neural_model
    .evaluate(X_test, y_test))
print(f"Neural Network Loss: {nn_loss:.2f}")
print(f"Neural Network Accuracy: {nn_accuracy*100:.2f}%")
print(f"Neural Network F1 Score: {nn_f1:.2f}")
print(f"Neural Network Balanced Error Rate: {nn_balanced_error_rate:.2f}")

# Make predictions with your Keras model
y_pred_prob = neural_model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Calculate the confusion matrix
nn_cm = confusion_matrix(y_test, y_pred)
print("Neural Net Confusion Matrix")
print(nn_cm)

# Create new figure for the Neural Net confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(nn_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Neural Net (keras) Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')

# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# Predict on the test set
y_pred = logistic_model.predict(X_test)

# Generate confusion matrix
lr_cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = lr_cm.ravel()
ber = (2 * max(fp, fn) / (tp + tn + fp + fn))

# Check the accuracy
print(f"Logistic Regression Precision: {precision_score(y_test, y_pred):.2f}")
print(f"Logistic Regression Recall: {recall_score(y_test, y_pred):.2f}")
print(f"Logistic Regression F1 Score: {f1_score(y_test, y_pred):.2f}")
print(f"Logistic Regression Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.2f}")
print(f"Logistic Regression Balanced Error Rate: {ber:.2f}")


print("Logistic Regression Confusion Matrix")
print(lr_cm)

# Create new figure for the Logistic Regression confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Logistic Regression (sklearn) Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')

# Display all figures
plt.show()
