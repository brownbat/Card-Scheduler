import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import seaborn as sns


# TODO csv read and preprocessing taking a very long time, find bottlenecks
# TODO MAJOR: currently just predicts all successful, or all fail,
#   based on sampling. it's like it "refuses to learn" and just guesses the
#   bigger class to maximize its reward. possibly a feature engineering or
#   hyperparameter tuning issue, but I **think** it's survivor bias, because
#   the dataset is already derived from an engine that's targeting a certain
#   success rate

def process_review_history(filename):
    '''Process the .csv export generated by Quantified Self add on into a
    pandas dataframe. The rows in this csv contain a span between reviews,
    so it lists review1 and review2, then the next row might be review2 and
    review3. Some processing is needed to ensure we capture each review,
    since most of them show up in the data twice, EXCEPT the first and last
    reviews only show up once, and in separate columns.
    '''
    df = pd.read_csv(filename)

    # create two dataframes from Date1, Answer1 and Date2, Answer2
    df_1 = df[['Date1', 'Answer1', 'Card ID']].copy()
    df_2 = df[['Date2', 'Answer2', 'Card ID']].copy()

    # rename columns to have same column names
    df_1.columns = ['Date', 'Answer', 'Card ID']
    df_2.columns = ['Date', 'Answer', 'Card ID']

    df_merged = pd.concat([df_1, df_2]).drop_duplicates()
    df_merged['Date'] = pd.to_datetime(df_merged['Date'])
    df_merged = df_merged.sort_values(['Card ID', 'Date'])
    df_merged = df_merged.reset_index(drop=True)

    return df_merged


def add_features(df):
    """Add additional features derived from collected data.
    """
    # Add Last date and interval
    df['Last Date'] = df.groupby('Card ID')['Date'].shift()
    df['Interval'] = df['Date'] - df['Last Date']

    # Add Last interval, Last result
    df['Last Interval'] = df.groupby('Card ID')['Interval'].shift()
    df['Last Answer'] = df.groupby('Card ID')['Answer'].shift()

    # Calculate the cumulative success rate.
    df['Successful'] = df['Answer'].apply(lambda x: 1 if x > 1 else 0)
    df['Cumulative Successes'] = df.groupby('Card ID')['Successful'].cumsum()
    df['Cumulative Reviews'] = df.groupby('Card ID').cumcount() + 1
    df['Cumulative Success Rate'] = (df['Cumulative Successes']
                                     / df['Cumulative Reviews'])

    # Add smoothed cumulative success rate
    # This might help minimize the issue of overconfidence at few reviews
    # TODO: tune additive smoothing values
    adsmooth_num = 0.85  # approx. collection overall success rate as prior
    adsmooth_den = 1
    df['Smoothed Success Rate'] = ((df['Cumulative Successes'] + adsmooth_num)
                                   / (df['Cumulative Reviews'] + adsmooth_den))

    # Add longest successful interval
    df['Longest Successful Interval'] = (
        df[df['Successful'] == True].groupby('Card ID')['Interval'].cummax()
    )

    df['Longest Successful Interval'] = (
        df.groupby('Card ID')['Longest Successful Interval']
        .fillna(method='ffill')
    )

    df['Longest Successful Interval'].fillna(pd.Timedelta(seconds=0),
                                             inplace=True)

    # Add shortest failed interval
    df['Shortest Failed Interval'] = (
        df[df['Successful'] == False].groupby('Card ID')['Interval'].cummin()
    )
    df['Shortest Failed Interval'] = (
        df.groupby('Card ID')['Shortest Failed Interval']
        .fillna(method='ffill')
    )

    return df


def normalize_dates(df):
    """Converts pd dates in a df dataframe to Unix epoch seconds"""
    df['Date'] = df['Date'].apply(lambda x: x.timestamp())
    df['Last Date'] = (
        df['Last Date']
        .apply(lambda x: x.timestamp() if pd.notnull(x) else x)
    )
    df['Interval'] = (
        df['Interval']
        .apply(lambda x: x.total_seconds() if pd.notnull(x) else x)
    )
    df['Last Interval'] = (
        df['Last Interval']
        .apply(lambda x: x.total_seconds() if pd.notnull(x) else x)
    )
    df['Longest Successful Interval'] = (
        df['Longest Successful Interval']
        .apply(lambda x: x.total_seconds() if pd.notnull(x) else x)
    )
    df['Shortest Failed Interval'] = (
        df['Shortest Failed Interval']
        .apply(lambda x: x.total_seconds() if pd.notnull(x) else x)
    )

    return df


def plot_interval_success(df):
    # plotting relationship between interval and success
    # interval bins are poorly labeled but are 1-2 days, 2-4 days, 4-8, etc

    # Convert 'Interval' from Timedelta to seconds
    df['Interval_seconds'] = df['Interval'].dt.total_seconds()

    # Then, create bins for 'Interval_seconds'
    bin_size = 60*60*24  # bin size of 1 day
    # bins = np.arange(0, df['Interval_seconds'].max() + bin_size, bin_size)

    bins = [0, (60*60*24)]
    while bins[-1] < df['Interval_seconds'].max():
        # Double the last value in the list and append it
        bins.append(bins[-1]*2)
    bins = np.array(bins)

    df['Interval_binned'] = pd.cut(df['Interval_seconds'], bins)

    # Then, group by 'Interval_binned' and calculate success rate
    success_rate = df.groupby('Interval_binned')['Successful'].mean()

    # Plot the success rate for each bin
    plt.figure(figsize=(10, 6))
    success_rate.plot(kind='line')
    plt.title('Success Rate vs Interval')
    plt.xlabel('Interval (days)')
    plt.ylabel('Success Rate')

    plt.show()


df = process_review_history('RevisionHistory.csv')
print('CSV read.')
df = add_features(df)
print('Features added.')
df = normalize_dates(df)
print('Dates normalized.')

'''
NaT errors to resolve:
Last Date: NaT if 1st review of card
Interval: NaT if 1st rev of card
Last Answer: NaT if 1st rev of card
Last Interval: NaT if no prior Interval, 1st or 2nd rev of card
Shortest Failed Interval: NaT until first fail
'''
# resolve NaT issues by removing first review of each card,
# removing the shortest failure column
# and imputing the last interval as this interval for second reviews
# cast Last Date and Interval to floats after NaTs removed
df = df.dropna(subset=['Last Date'])
df['Last Interval'].fillna(df['Interval'], inplace=True)
df.drop('Shortest Failed Interval', axis=1, inplace=True)

df['Last Date'] = df['Last Date'].astype(float)
df['Interval'] = df['Interval'].astype(float)

print("NaT issues resolved.")

# Force display of all rows and columns
# pd.set_option('display.max_rows', 1000)
# pd.set_option('display.max_columns', 1000)
# pd.set_option('display.width', 1000)
# print(df.head(50))


# Balance df
# Determine target ratio
successful_ratio = 0.505
unsuccessful_ratio = 1 - successful_ratio

# Count of 'unsuccessful' reviews
num_unsuccessful = df['Successful'].value_counts()[0]

# Calculate target number of 'successful' reviews based on the target ratio
target_successful = int(num_unsuccessful * (successful_ratio / unsuccessful_ratio))

# Get indices of 'successful' and 'unsuccessful' reviews
successful_indices = df[df['Successful'] == 1].index
unsuccessful_indices = df[df['Successful'] == 0].index

# Randomly select 'successful' reviews to reach target number
random_indices = np.random.choice(successful_indices, target_successful, replace=False)

# Concatenate indices and create a balanced dataframe
under_sample_indices = np.concatenate([unsuccessful_indices, random_indices])
df_balanced = df.loc[under_sample_indices]

print("Dataframe balanced with {}% successful reviews.".format(successful_ratio * 100))


X = df_balanced.drop('Successful', axis=1)
y = df_balanced['Successful']

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42)

print("Training and testing sets split.")


# Define a neural net model using keras
# TODO: Tune num layers and neurons (grid search?).
# honestly tuning is optimistic because nothing seems to produce any other
# result at the moment
neural_model = Sequential()
# First hidden layer with 64 neurons
neural_model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
# Second hidden layer with 32 neurons
neural_model.add(Dense(32, activation='relu'))
# other layers
neural_model.add(Dense(32, activation='relu'))
neural_model.add(Dense(1, activation='sigmoid'))  # Output layer

# Compile the model
neural_model.compile(loss='binary_crossentropy',
                     optimizer='adam',
                     metrics=['accuracy'])

# Fit the model
neural_model.fit(X_train, y_train, epochs=15, batch_size=32)

# Evaluate the model
_, accuracy = neural_model.evaluate(X_test, y_test)
print('Neural Network Accuracy: %.2f' % (accuracy*100))


# Make predictions with your Keras model
y_pred_prob = neural_model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)

# Calculate the confusion matrix
nn_cm = confusion_matrix(y_test, y_pred)

# Create new figure for the Neural Net confusion matrix
plt.figure(figsize=(10,7))
sns.heatmap(nn_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Neural Net (keras) Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')

# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# Predict on the test set
logistic_predictions = logistic_model.predict(X_test)

# Check the accuracy
print("Logistic Regression Accuracy: ",
      accuracy_score(y_test, logistic_predictions))

# Make predictions on your test data
y_pred = logistic_model.predict(X_test)

# Generate confusion matrix
lr_cm = confusion_matrix(y_test, y_pred)

print(lr_cm)

# Create new figure for the Logistic Regression confusion matrix
plt.figure(figsize=(10,7))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues')
plt.title('Logistic Regression (sklearn) Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')

# Display all figures
plt.show()
